\documentclass[a4paper, 12pt]{report}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}

\usepackage[top=3cm, bottom=3cm, left=3cm, right=3cm]{geometry}
\usepackage{setspace}
\usepackage{soul}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{rotating}
\usepackage{titling}
\usepackage{hyperref}
\usepackage{pdfpages}
\usepackage{float}
% Algorithms
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[]{algorithm2e}
\usepackage{moreverb}
% Bibliography
\usepackage[nottoc]{tocbibind}

\lstset{upquote=true,
        columns=flexible,
        keepspaces=true,
        breaklines,
        breakindent=0pt,
        basicstyle=\ttfamily,
        commentstyle=\itshape,
        keywordstyle=\bfseries,
        language=Matlab,
        texcsstyle=*\bfseries}

\postdate{
  \begin{center} 
    \vspace*{3cm}
    \includegraphics[width=10cm]{logo_Poly.jpg}
  \end{center}
}

\newcommand{\ora}[1]{\overrightarrow{#1}}
\newcommand{\R}{\mathbb{R}}

\begin{document}

\def\labelitemi{$\bullet$}

\title{{\Large \textbf{Artificial Intelligence: Probabilistic \& learning techniques\\ } \\Chris \bsc{Pal} \\ Project (INF8225)}}
\author{Farnoush \bsc{Farhadi} \& Juliette \bsc{Tibayrenc}} 
\date{21 Apr. 2016}

\maketitle

\setcounter{tocdepth}{5}

\tableofcontents

\newpage

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

Our AI course has allowed us to discover Markov Decision Processes, which we decided to further explore
in our project.
Looking for an application for them, we stumbled upon Mastronarde \& Van der Schaar's article,
`Fast Reinforcement Learning for Energy-Efficient Wireless Communications'\cite{Mastronarde-2011},
and we chose it as the basis for our work.

The article presents the two researchers' work in developing an algorithm that would enable systems
to adopt the best policy to quickly send data packets in the most efficient way possible,
an important goal to achieve since the majority of today's surfing is done on smartphones
whose battery does not last longer than a day (at best).

In this report, we will first introduce the article, with a quick review of the state of the art
at the time it was written (2011), then summarize it.
We will continue with a presentation of some theoretical elements, highlighting the differences between
what we saw in class and the processes on which the article and our implementations are based,
and the way the reward matrix used is calculated (a process that does not appear in the article).
Finally we will talk about our own implementation of classic Q-learning and one of its variants,
speedy Q-learning, and compare the results we obtain with the results obtained with the authors'algorithm,
plus the results we get by using value iteration and policy iteration.

\chapter*{1. Reference article}
\addcontentsline{toc}{chapter}{1. Reference article}

\section*{1.1 Literature review}
\addcontentsline{toc}{section}{1.1 Literature review}

\section*{1.2 Article summary}
\addcontentsline{toc}{section}{1.2 Article summary}

\chapter*{2. Theoretical elements}
\addcontentsline{toc}{chapter}{2. Theoretical elements}

\section*{2.1 Value iterating \& policy iterating vs. Q-learning}
\addcontentsline{toc}{section}{2.1 Value iterating \& policy iterating vs. Q-learning}

In our project, we compare the performance of several algorithms against that of the authors.
These algorithms have to be evaluated along several criteria, among which the elements that need to be known
for them to work, the speed with which they are executed (partly dependent on the efficency of the implementation and the language used)
and the number of iterations their internal loop is executed for.

These algorithms are value iteration, policy iteration, classic Q-learning and speedy Q-learning.

\subsection*{2.1.1 Value iteration}
\addcontentsline{toc}{subsection}{2.1.1 Value iteration}


\subsection*{2.1.2 Policy iteration}
\addcontentsline{toc}{subsection}{2.1.2 Policy iteration}

\subsection*{2.1.3 Classic Q-learning}
\addcontentsline{toc}{subsection}{2.1.3 Classic Q-learning}

Q-learning is part of a second category of algorithms that do not presuppose the full knowledge of the system.
Contrary to value iteration and policy iteration, the environment does not need to be known for the algorithm to work.
The algorithm presented by the authors of our reference article is similar in its requirements, though it goes further and
actually uses the knowledge we do have about the environment, which makes it more efficient.

Q-learning helps us find the optimal policy by using the interactions between the system and the environment.
In it, we compute an estimate of the cumulative discounted reward for each action executed in each state.

The algorithm for one step is as follows (from Artificial Intelligence, 3rd edition, French version\cite{Russell-2010}:

\begin{algorithm}[H]
  \KwData{$s'$ /* current state */, $r'$ /* reward signal */}
  \KwResult{$Q$ /* action values table indexed by states and actions */, $T$ /* frequency of state-action couples */, $s,a,r$ /*previous state, action and reward */}
  \If{s final}{
    $Q(s,Empty) = r'$
  }
  \If{$s\neq0$}{
    $T(s,a)++$\\
    $Q(s,a)+=\alpha\cdot T(s,a)(r+\gamma\max_{a'}Q(s',a')-Q(s,a))$
  }
  $s,a,r=s',\argmax_{a'}f(Q(s',a'),T(s',a')),r'$\\
  return $a$
\caption{Q-learning algorithm for one step}
\end{algorithm}

\section*{2.2 Reward matrix}
\addcontentsline{toc}{section}{2.2 Reward matrix}

\chapter*{3. Comparing algorithms}
\addcontentsline{toc}{chapter}{3. Comparing algorithms}

\section*{3.1 Classic Q-learning}
\addcontentsline{toc}{section}{3.1 Classic Q-learning}

\section*{3.2 Speedy Q-learning}
\addcontentsline{toc}{section}{3.2 Speedy Q-learning}

\section*{3.3 Additional remarks}
\addcontentsline{toc}{section}{3.3 Additional remarks}

\chapter*{Further work \& conclusion}
\addcontentsline{toc}{chapter}{Further work and conclusion}

\bibliographystyle{ieeetr}
\bibliography{report}


\end{document}
