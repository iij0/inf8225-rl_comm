\documentclass[a4paper, 12pt]{report}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}

\usepackage[top=3cm, bottom=3cm, left=3cm, right=3cm]{geometry}
\usepackage{setspace}
\usepackage{soul}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{rotating}
\usepackage{titling}
\usepackage{hyperref}
\usepackage{pdfpages}
\usepackage{float}
% Algorithms
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[]{algorithm2e}
\usepackage{moreverb}
% Bibliography
\usepackage[nottoc]{tocbibind}

\lstset{upquote=true,
        columns=flexible,
        keepspaces=true,
        breaklines,
        breakindent=0pt,
        basicstyle=\ttfamily,
        commentstyle=\itshape,
        keywordstyle=\bfseries,
        language=Matlab,
        texcsstyle=*\bfseries}

\postdate{
  \begin{center} 
    \vspace*{3cm}
    \includegraphics[width=10cm]{logo_Poly.jpg}
  \end{center}
}

\newcommand{\ora}[1]{\overrightarrow{#1}}
\newcommand{\R}{\mathbb{R}}

\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}

\begin{document}

\def\labelitemi{$\bullet$}

\title{{\Large \textbf{Artificial Intelligence: Probabilistic \& learning techniques\\ } \\Chris \bsc{Pal} \\ Project (INF8225)}}
\author{Farnoush \bsc{Farhadi} \& Juliette \bsc{Tibayrenc}} 
\date{21 Apr. 2016}

\maketitle

\setcounter{tocdepth}{5}

\tableofcontents

\newpage

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

Our AI course has allowed us to discover Markov Decision Processes, which we decided to further explore
in our project.
Looking for an application for them, we stumbled upon Mastronarde \& Van der Schaar's article,
`Fast Reinforcement Learning for Energy-Efficient Wireless Communications'\cite{Mastronarde-2011},
and we chose it as the basis for our work.

The article presents the two researchers' work in developing an algorithm that would enable systems
to adopt the best policy to quickly send data packets in the most efficient way possible,
an important goal to achieve since the majority of today's surfing is done on smartphones
whose battery does not last longer than a day (at best).

In this report, we will first introduce the article, with a quick review of the state of the art
at the time it was written (2011), then summarize it.
We will continue with a presentation of some theoretical elements, highlighting the differences between
what we saw in class and the processes on which the article and our implementations are based,
and the way the reward matrix used is calculated (a process that does not appear in the article).
Finally we will talk about our own implementation of classic Q-learning and one of its variants,
speedy Q-learning, and compare the results we obtain with the results obtained with the authors'algorithm,
plus the results we get by using value iteration and policy iteration.

\chapter*{1. Reference article}
\addcontentsline{toc}{chapter}{1. Reference article}

\section*{1.1 Literature review}
\addcontentsline{toc}{section}{1.1 Literature review}

\section*{1.2 Article summary}
\addcontentsline{toc}{section}{1.2 Article summary}

\chapter*{2. Theoretical elements}
\addcontentsline{toc}{chapter}{2. Theoretical elements}

\section*{2.1 Value iterating \& policy iterating vs. Q-learning}
\addcontentsline{toc}{section}{2.1 Value iterating \& policy iterating vs. Q-learning}

In our project, we compare the performance of several algorithms against that of the authors.
These algorithms have to be evaluated along several criteria, among which the elements that need to be known
for them to work, the speed with which they are executed (partly dependent on the efficency of the implementation and the language used)
and the number of iterations their internal loop is executed for.

These algorithms are value iteration, policy iteration, classic Q-learning and speedy Q-learning.

\subsection*{2.1.1 Value iteration}
\addcontentsline{toc}{subsection}{2.1.1 Value iteration}

One method to find an optimal policy is to find the optimal value function by determining a simple iterative algorithm called value iteration.
The value iteration method is referenced by Bellman equation for MDP problems which is obtained simply by turning the Bellman optimality equation into an update rule.
If there exist $n$ possible states, then we have $n$ Bellman equations, one per each state.
The $n$ equations contain $n$ unknowns which are the utilities of theses states. 

We aim to solve these simultaneous equations to find the utility values.
These equations are non-linear, since the max operator is not a linear one.
Hence, it can be determined by a simple iterative algorithm that can be shown as follows:

\[V_{i+1}(s) = R(s) + \gamma \mathrm{max_a} \sum_s P(s,a,s') V_{i}(s')\]

where $i$ is the iteration number.
Value iteration starts at $i = 0$ and then iterates, repeatedly computing $V_{i+1}$ for all states s, until V converges.
Value iteration formally requires an infinite number of iteration to converge to $V^*$.
In practice, it stops once the value function changes by a small amount in a sweep which satisfies the Bellman equations.
This process is called a value update or Bellman update/backup. 
The algorithm for value iteration is as follows (from Artificial Intelligence, 3rd edition, French version \cite{Russell-2010}):

  
\begin{algorithm} \caption{Iterative Value Algorithm}
\begin{algorithmic}[H]
\State \textbf{Data:  $P(s,a,s')$ /* a transition matrix */, $a$ /* actions */, $R(s)$ /* reward signal */, $\gamma$ /* discount*/, $\epsilon$ /* the maximum error allowed in the utility of any state */}
\State \textbf{Local Variables: $V$ , /* Value matrix, initially identical to $R(s)$ */\\
$V^{'}$, /* Value matrix, initially identical to $R(s)$ */}

\State \textbf{Result:  } $V(s)$ /* Value function */
\State \textbf{Initilize: } 
\State $V(s) \gets \texttt{arbitrarily}$, for all $s \in \mathcal S^+$
\Repeat
%\State $V \gets V'$
\ForEach {$\texttt{state  }$ $s \in \mathcal S^+$}
\State $V^{'}(s) \gets R(s) + \gamma \mathrm{max_a} \sum_{s'} P(s,a,s') V(s')$
\State $\Delta \gets \max(\Delta, V(s) - V'(s)|)$
\EndFor
\Until {$\Delta < \epsilon$}
\State $\textbf{Return } V(s)$
\end{algorithmic}
\end{algorithm}

Note that the value iteration backup is identical to the policy evaluation except that it requires the max operator to be taken over all actions.

\subsection*{2.1.2 Policy iteration}
\addcontentsline{toc}{subsection}{2.1.2 Policy iteration}

In policy iteration algorithm, the agent manipulates the policy directly, instead of finding it indirectly via the optimal value function. 
The policy often becomes exactly optimal long before the utility estimation have converged to their true values. This phenomena leads us to another way of finding optimal policies, called policy iteration. Policy iteration picks an initial action arbitrarily by taking rewards on states as
their utilities and computing a policy according to the maximum expected
utility. Then it iteratively performs two steps: firstly, it determine the values through
computing the utility of each state given the current action, and then policy improvement by updating the current policy if possible. 
To detect when the algorithm has converged, it should only change the policy if the new action for some state improves the expected value; that is, it should stabilize the policy.
The algorithm for policy iteration is as follows:

\begin{algorithm}
\caption{Iterative Policy Algorithm}
\begin{algorithmic}[H]
\State $\textbf{Data:  $P(s,a,s')$ /* a transition matrix */, $a$ /* actions */, $R(s)$ /* reward signal */, $\gamma$ /* discount*/, $\epsilon$ /* the maximum error allowed in the utility of any state */}
\State $\textbf{Local Variables:  $V$ , /* Value matrix, initially identical to $R(s)$ */\\$\pi$, /* a policy, initially optimal respecting to $V$ */}
\State $\textbf{Result:  $V(s)$ /* Value function */}

\State $\textbf{Initilize: }
\State $V(s) \gets \texttt{arbitrarily}$, for all $s \in \mathcal S^+$
\State $\pi(s) \gets \texttt{arbitrarily}$, for all $s \in \mathcal S^+$
\State $\mathrm{Policy Evaluation:}$
\State $V \gets V^{'}$
\ForEach {$\texttt{state  }$ $s \in \mathcal S^+$}
\State $V^{'}(s) \gets R(s) + \gamma \mathrm{max_a} \sum_{s'} P(s,a,s') V(s')$
\State $\Delta \gets \max(\Delta, V(s) - V^{'}(s)|)$
\EndFor
\Until {$\Delta < \epsilon$}
\State $\mathrm{Policy Improvement:}$
\State $\mathrm{stable == True}$
\ForEach {$\texttt{state  }$ $s \in \mathcal S^+$}
\State $a \gets \pi(s)$
\State $\pi(s) \gets \mathrm{argmax}_a \sum_{s'} P(s,a,s')V(s')$
\If{$ a \neq \pi(s)$}
\State $\mathrm{stable == False}$
\EndFor
\end{algorithmic}
\end{algorithm}

At the end of algorithm, we test the stable variable; if it is 'True', algorithm terminates, otherwise, control jumps to the Policy Evaluation and continues in an iterative manner. 
Policy iteration often converges in few iterations. The policy improvement theorem guarantees that these policies works better than the original random policy and achieves to the goal states in the minimum number of steps.

\subsection*{2.1.3 Classic Q-learning}
\addcontentsline{toc}{subsection}{2.1.3 Classic Q-learning}

Q-learning is part of a second category of algorithms that do not presuppose the full knowledge of the system.
Contrary to value iteration and policy iteration, the environment does not need to be known for the algorithm to work.
The algorithm presented by the authors of our reference article is similar in its requirements, though it goes further and
actually uses the knowledge we do have about the environment, which makes it more efficient.

Q-learning helps us find the optimal policy by using the interactions between the system and the environment.
In it, we compute an estimate of the cumulative discounted reward for each action executed in each state.

The algorithm for one step is as follows (from Artificial Intelligence, 3rd edition, French version\cite{Russell-2010}:

\begin{algorithm}[H]
  \KwData{$s'$ /* current state */, $r'$ /* reward signal */}
  \KwResult{$Q$ /* action values table indexed by states and actions */, $T$ /* frequency of state-action couples */, $s,a,r$ /*previous state, action and reward */}
  \If{s final}{
    $Q(s,Empty) = r'$
  }
  \If{$s\neq0$}{
    $T(s,a)++$\\
    $Q(s,a)+=\alpha\cdot T(s,a)(r+\gamma\max_{a'}Q(s',a')-Q(s,a))$
  }
  $s,a,r=s',\argmax_{a'}f(Q(s',a'),T(s',a')),r'$\\
  return $a$
\caption{Q-learning algorithm for one step}
\end{algorithm}

\section*{2.2 Reward matrix}
\addcontentsline{toc}{section}{2.2 Reward matrix}

\chapter*{3. Comparing algorithms}
\addcontentsline{toc}{chapter}{3. Comparing algorithms}

\section*{3.1 Classic Q-learning}
\addcontentsline{toc}{section}{3.1 Classic Q-learning}

\section*{3.2 Speedy Q-learning}
\addcontentsline{toc}{section}{3.2 Speedy Q-learning}

\section*{3.3 Additional remarks}
\addcontentsline{toc}{section}{3.3 Additional remarks}

\chapter*{Further work \& conclusion}
\addcontentsline{toc}{chapter}{Further work and conclusion}

\bibliographystyle{ieeetr}
\bibliography{report}


\end{document}
