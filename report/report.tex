\documentclass[a4paper, 12pt]{report}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[francais]{babel}

\usepackage[top=3cm, bottom=3cm, left=3cm, right=3cm]{geometry}
\usepackage{setspace}
\usepackage{soul}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{rotating}
\usepackage{titling}
\usepackage{hyperref}
\usepackage{pdfpages}
\usepackage{float}

\usepackage[nottoc]{tocbibind}

\postdate{
  \begin{center} 
    \vspace*{3cm}
    \includegraphics[width=10cm]{logo_Poly.jpg}
  \end{center}
}

\newcommand{\ora}[1]{\overrightarrow{#1}}
\newcommand{\R}{\mathbb{R}}

\begin{document}

\def\labelitemi{$\bullet$}

\title{{\Large \textbf{Artificial Intelligence: Probabilistic \& learning techniques\\ } \\Chris \bsc{Pal} \\ Project (INF8225)}}
\author{Farnoush \bsc{Farhadi} \& Juliette \bsc{Tibayrenc}} 
\date{21 Apr. 2016}

\maketitle

\setcounter{tocdepth}{5}

\tableofcontents

\newpage

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

Our AI course has allowed us to discover Markov Decision Processes, which we decided to further explore
in our project.
Looking for an application for them, we stumbled upon Mastronarde \& Van der Schaar's article,
`Fast Reinforcement Learning for Energy-Efficient Wireless Communications'\cite{Mastronarde-2011},
and we chose it as the basis for our work.

The article presents the two researchers' work in developing an algorithm that would enable systems
to adopt the best policy to quickly send data packets in the most efficient way possible,
an important goal to achieve since the majority of today's surfing is done on smartphones
whose battery does not last longer than a day (at best).

In this report, we will first introduce the article, with a quick review of the state of the art
at the time it was written (2011), then summarize it.
We will continue with a presentation of some theoretical elements, highlighting the differences between
what we saw in class and the processes on which the article and our implementations are based,
and the way the reward matrix used is calculated (a process that does not appear in the article).
Finally we will talk about our own implementation of classic Q-learning and one of its variants,
speedy Q-learning, and compare the results we obtain with the results obtained with the authors'algorithm,
plus the results we get by using value iteration and policy iteration.

\chapter*{1. Reference article}
\addcontentsline{toc}{chapter}{1. Reference article}

\section*{1.1 Literature review}
\addcontentsline{toc}{section}{1.1 Literature review}

\section*{1.2 Article summary}
\addcontentsline{toc}{section}{1.2 Article summary}

\chapter*{2. Theoretical elements}
\addcontentsline{toc}{chapter}{2. Theoretical elements}

\section*{2.1 Value iterating \& policy iterating vs. Q-learning}
\addcontentsline{toc}{section}{2.1 Value iterating \& policy iterating vs. Q-learning}

\section*{2.2 Reward matrix}
\addcontentsline{toc}{section}{2.2 Reward matrix}

\chapter*{3. Comparing algorithms}
\addcontentsline{toc}{chapter}{3. Comparing algorithms}

\section*{3.1 Classic Q-learning}
\addcontentsline{toc}{section}{3.1 Classic Q-learning}

\section*{3.2 Speedy Q-learning}
\addcontentsline{toc}{section}{3.2 Speedy Q-learning}

\section*{3.3 Additional remarks}
\addcontentsline{toc}{section}{3.3 Additional remarks}

\chapter*{Further work \& conclusion}
\addcontentsline{toc}{chapter}{Further work and conclusion}

\bibliographystyle{ieeetr}
\bibliography{report}


\end{document}
